"""
Sentinel-2 Time Series Building Classification
VERSION 06 - STRICT NODATA HANDLING + MONTH DELETION

Key Features:
- Two-pass approach: Download all ‚Üí Find valid patches ‚Üí Filter months ‚Üí Classify
- STRICT nodata checking: Any NaN or zero = invalid patch
- Months with fewer valid patches than maximum are DELETED
- All remaining months guaranteed to have identical valid patch coverage
- Reports: X out of Y months have valid cloud-free data
- Fixed 10% cloud cover threshold
- Full cache support with resume capability
"""

import os
os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

import numpy as np
import matplotlib.pyplot as plt
from shapely.geometry import Polygon, box
import rasterio
from rasterio.warp import calculate_default_transform, reproject, Resampling
from patchify import patchify, unpatchify
import datetime
import torch
import math
from scipy import ndimage
import ee
import tempfile
from pathlib import Path
import requests
import time
import io
import warnings
import sys
import base64
import json
import subprocess
from datetime import date
from PIL import Image

warnings.filterwarnings("ignore", category=UserWarning)
warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", message="Examining the path of torch.classes")

import streamlit as st

st.set_page_config(
    layout="wide", 
    page_title="Building Classification Time Series v06",
    page_icon="üèóÔ∏è"
)

import folium
from folium import plugins
from streamlit_folium import st_folium
import segmentation_models_pytorch as smp

# =============================================================================
# CONSTANTS
# =============================================================================
SPECTRAL_BANDS = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7', 'B8', 'B8A', 'B9', 'B11', 'B12']
PATCH_SIZE = 224

# RGB bands for visualization (True Color: B4=Red, B3=Green, B2=Blue)
RGB_BAND_INDICES = {'red': 3, 'green': 2, 'blue': 1}

# Download settings
MAX_RETRIES = 3
RETRY_DELAY_BASE = 2
DOWNLOAD_TIMEOUT = 120
CHUNK_SIZE = 8192

# Minimum expected file sizes for validation
MIN_BAND_FILE_SIZE = 10000
MIN_MULTIBAND_FILE_SIZE = 100000

# FIXED cloud cover threshold (no slider)
CLOUDY_PIXEL_PERCENTAGE = 10

# =============================================================================
# Session State Initialization
# =============================================================================
if 'drawn_polygons' not in st.session_state:
    st.session_state.drawn_polygons = []
if 'last_drawn_polygon' not in st.session_state:
    st.session_state.last_drawn_polygon = None
if 'ee_initialized' not in st.session_state:
    st.session_state.ee_initialized = False
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'model' not in st.session_state:
    st.session_state.model = None
if 'device' not in st.session_state:
    st.session_state.device = None
if 'classification_thumbnails' not in st.session_state:
    st.session_state.classification_thumbnails = []
if 'processing_complete' not in st.session_state:
    st.session_state.processing_complete = False
if 'data_summary' not in st.session_state:
    st.session_state.data_summary = None
if 'processed_months' not in st.session_state:
    st.session_state.processed_months = {}
if 'failed_months' not in st.session_state:
    st.session_state.failed_months = []
if 'current_temp_dir' not in st.session_state:
    st.session_state.current_temp_dir = None
if 'processing_in_progress' not in st.session_state:
    st.session_state.processing_in_progress = False
if 'last_processed_index' not in st.session_state:
    st.session_state.last_processed_index = 0
# Valid patch mask and downloaded images tracking
if 'valid_patches_mask' not in st.session_state:
    st.session_state.valid_patches_mask = None
if 'downloaded_images' not in st.session_state:
    st.session_state.downloaded_images = {}
if 'download_phase_complete' not in st.session_state:
    st.session_state.download_phase_complete = False
if 'validity_analysis_complete' not in st.session_state:
    st.session_state.validity_analysis_complete = False
# NEW: Track months that passed validation
if 'valid_months' not in st.session_state:
    st.session_state.valid_months = []
if 'deleted_months' not in st.session_state:
    st.session_state.deleted_months = []
if 'total_requested_months' not in st.session_state:
    st.session_state.total_requested_months = 0


# =============================================================================
# Normalization function - matches working app exactly
# =============================================================================
def normalized(img):
    """
    Normalize image data to range [0, 1]
    This normalizes the ENTIRE array globally (all bands together)
    """
    min_val = np.nanmin(img)
    max_val = np.nanmax(img)
    
    if max_val == min_val:
        return np.zeros_like(img)
    
    img_norm = (img - min_val) / (max_val - min_val)
    return img_norm


# =============================================================================
# File Validation Functions
# =============================================================================
def validate_geotiff_file(file_path, expected_bands=1):
    """Validate that a GeoTIFF file is complete and readable."""
    try:
        if not os.path.exists(file_path):
            return False, "File does not exist"
        
        file_size = os.path.getsize(file_path)
        min_size = MIN_BAND_FILE_SIZE if expected_bands == 1 else MIN_MULTIBAND_FILE_SIZE
        
        if file_size < min_size:
            return False, f"File too small ({file_size} bytes, expected > {min_size})"
        
        with rasterio.open(file_path) as src:
            if src.count < expected_bands:
                return False, f"Wrong band count ({src.count}, expected {expected_bands})"
            
            for band_idx in range(1, min(src.count + 1, expected_bands + 1)):
                window = rasterio.windows.Window(0, 0, min(10, src.width), min(10, src.height))
                data = src.read(band_idx, window=window)
                if np.all(np.isnan(data)):
                    return False, f"Band {band_idx} contains only NaN values"
        
        return True, "File is valid"
        
    except rasterio.errors.RasterioIOError as e:
        return False, f"Rasterio cannot read file: {str(e)}"
    except Exception as e:
        return False, f"Validation error: {str(e)}"


def validate_band_file(band_file_path, band_name):
    """Validate a single band GeoTIFF file."""
    return validate_geotiff_file(band_file_path, expected_bands=1)


# =============================================================================
# Model Download Functions
# =============================================================================
@st.cache_data
def download_model_from_gdrive(gdrive_url, local_filename):
    """Download a file from Google Drive with improved error handling"""
    try:
        correct_file_id = "1m6EScw-mpBIvWV78h4pyjWq1OLQtn2ov"
        st.info(f"Downloading model from Google Drive (File ID: {correct_file_id})...")
        
        try:
            import gdown
        except ImportError:
            st.info("Installing gdown library...")
            subprocess.check_call([sys.executable, "-m", "pip", "install", "gdown"])
            import gdown
        
        download_methods = [
            f"https://drive.google.com/uc?id={correct_file_id}",
            f"https://drive.google.com/file/d/{correct_file_id}/view",
            correct_file_id
        ]
        
        for i, method in enumerate(download_methods):
            try:
                st.info(f"Trying download method {i+1}/3...")
                gdown.download(method, local_filename, quiet=False, fuzzy=True)
                
                if os.path.exists(local_filename) and os.path.getsize(local_filename) > 1024:
                    file_size = os.path.getsize(local_filename)
                    with open(local_filename, 'rb') as f:
                        header = f.read(10)
                        if header.startswith(b'\x80\x02') or header.startswith(b'\x80\x03') or header.startswith(b'PK'):
                            st.success(f"Model downloaded successfully! Size: {file_size / (1024*1024):.1f} MB")
                            return local_filename
                        else:
                            if os.path.exists(local_filename):
                                os.remove(local_filename)
                else:
                    if os.path.exists(local_filename):
                        os.remove(local_filename)
                        
            except Exception as e:
                st.warning(f"Download method {i+1} failed: {str(e)}")
                if os.path.exists(local_filename):
                    os.remove(local_filename)
                continue
        
        return manual_download_fallback(correct_file_id, local_filename)
            
    except Exception as e:
        st.error(f"Error in download function: {str(e)}")
        return None


def manual_download_fallback(file_id, local_filename):
    """Fallback manual download method using requests"""
    try:
        urls_to_try = [
            f"https://drive.google.com/uc?export=download&id={file_id}",
            f"https://drive.google.com/uc?export=download&id={file_id}&confirm=t",
            f"https://drive.usercontent.google.com/download?id={file_id}&export=download",
        ]
        
        session = requests.Session()
        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
        
        for i, url in enumerate(urls_to_try):
            try:
                st.info(f"Trying manual method {i+1}/3...")
                response = session.get(url, headers=headers, stream=True, timeout=30)
                
                if response.status_code == 200:
                    content_type = response.headers.get('content-type', '').lower()
                    
                    if 'text/html' not in content_type:
                        total_size = int(response.headers.get('content-length', 0))
                        downloaded_size = 0
                        
                        if total_size > 0:
                            progress_bar = st.progress(0)
                            status_text = st.empty()
                        
                        with open(local_filename, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                if chunk:
                                    f.write(chunk)
                                    downloaded_size += len(chunk)
                                    if total_size > 0:
                                        progress = downloaded_size / total_size
                                        progress_bar.progress(progress)
                                        status_text.text(f"Downloaded: {downloaded_size / (1024*1024):.1f} MB / {total_size / (1024*1024):.1f} MB")
                        
                        if total_size > 0:
                            progress_bar.empty()
                            status_text.empty()
                        
                        if os.path.exists(local_filename) and os.path.getsize(local_filename) > 1024:
                            file_size = os.path.getsize(local_filename)
                            st.success(f"Manual download successful! Size: {file_size / (1024*1024):.1f} MB")
                            return local_filename
                        
            except Exception as e:
                st.warning(f"Manual method {i+1} failed: {e}")
                continue
        
        st.error("All automatic download methods failed. Please download manually:")
        st.markdown(f"""
        1. **Open this link:** https://drive.google.com/file/d/{file_id}/view
        2. **Click the Download button**
        3. **Save the file as:** `{local_filename}`
        4. **Upload it using the file uploader below**
        """)
        
        uploaded_file = st.file_uploader(
            f"Upload the model file ({local_filename}) after manual download:",
            type=['pt', 'pth']
        )
        
        if uploaded_file is not None:
            with open(local_filename, 'wb') as f:
                f.write(uploaded_file.read())
            file_size = os.path.getsize(local_filename)
            st.success(f"Model uploaded successfully! Size: {file_size / (1024*1024):.1f} MB")
            return local_filename
        
        return None
        
    except Exception as e:
        st.error(f"Manual download fallback failed: {e}")
        return None


# =============================================================================
# Model Loading Function
# =============================================================================
@st.cache_resource
def load_model(model_path):
    """Load the UNet++ model for building detection"""
    try:
        device = torch.device('cpu')
        model = smp.UnetPlusPlus(
            encoder_name='timm-efficientnet-b7',
            encoder_weights='imagenet',
            in_channels=12,
            classes=1,
            decoder_attention_type='scse'
        ).to(device)

        loaded_object = torch.load(model_path, map_location=device, weights_only=False)

        if isinstance(loaded_object, dict) and 'model_state_dict' in loaded_object:
            model.load_state_dict(loaded_object['model_state_dict'])
            st.info("Model loaded from checkpoint dictionary.")
        elif isinstance(loaded_object, dict):
            model.load_state_dict(loaded_object)
            st.info("Model loaded directly from state_dict.")
        else:
            st.error("Loaded model file is not a recognized state_dict or checkpoint format.")
            return None, None

        model.eval()
        st.success("Model loaded successfully!")
        return model, device
    except Exception as e:
        st.error(f"Error loading model: {str(e)}")
        return None, None


# =============================================================================
# Earth Engine Authentication
# =============================================================================
@st.cache_resource
def initialize_earth_engine():
    """Initialize Earth Engine with authentication."""
    try:
        ee.Initialize()
        return True, "Earth Engine already initialized"
    except Exception:
        try:
            base64_key = os.environ.get('GOOGLE_EARTH_ENGINE_KEY_BASE64')
            
            if base64_key:
                key_json = base64.b64decode(base64_key).decode()
                key_data = json.loads(key_json)
                
                key_file = tempfile.NamedTemporaryFile(suffix='.json', delete=False)
                with open(key_file.name, 'w') as f:
                    json.dump(key_data, f)
                
                credentials = ee.ServiceAccountCredentials(
                    key_data['client_email'],
                    key_file.name
                )
                ee.Initialize(credentials)
                os.unlink(key_file.name)
                return True, "Successfully authenticated with Earth Engine (Service Account)!"
            else:
                ee.Authenticate()
                ee.Initialize()
                return True, "Successfully authenticated with Earth Engine!"
        except Exception as auth_error:
            return False, f"Authentication failed: {str(auth_error)}"


# =============================================================================
# Helper Functions
# =============================================================================
def get_utm_zone(longitude):
    """Determine the UTM zone for a given longitude."""
    return math.floor((longitude + 180) / 6) + 1

def get_utm_epsg(longitude, latitude):
    """Determine the EPSG code for UTM zone based on longitude and latitude."""
    zone_number = get_utm_zone(longitude)
    if latitude >= 0:
        return f"EPSG:326{zone_number:02d}"
    else:
        return f"EPSG:327{zone_number:02d}"


# =============================================================================
# RGB Image Generation Functions
# =============================================================================
def generate_rgb_thumbnail(image_path, month_name, max_size=256):
    """
    Generate an RGB thumbnail from a Sentinel-2 multiband image.
    Uses B4 (Red), B3 (Green), B2 (Blue) for true color visualization.
    """
    try:
        with rasterio.open(image_path) as src:
            red = src.read(4)    # B4 - Red
            green = src.read(3)  # B3 - Green
            blue = src.read(2)   # B2 - Blue
            
            rgb = np.stack([red, green, blue], axis=-1)
            rgb = np.nan_to_num(rgb, nan=0.0)
            
            def percentile_stretch(band, lower=2, upper=98):
                """Apply percentile stretching to enhance contrast"""
                valid = band[band > 0]
                if len(valid) == 0:
                    return np.zeros_like(band, dtype=np.uint8)
                p_low = np.percentile(valid, lower)
                p_high = np.percentile(valid, upper)
                
                if p_high <= p_low:
                    p_high = p_low + 0.001
                
                stretched = np.clip((band - p_low) / (p_high - p_low), 0, 1)
                return (stretched * 255).astype(np.uint8)
            
            rgb_uint8 = np.zeros_like(rgb, dtype=np.uint8)
            for i in range(3):
                rgb_uint8[:, :, i] = percentile_stretch(rgb[:, :, i])
            
            pil_img = Image.fromarray(rgb_uint8, mode='RGB')
            
            h, w = pil_img.size[1], pil_img.size[0]
            if h > max_size or w > max_size:
                scale = max_size / max(h, w)
                new_h, new_w = int(h * scale), int(w * scale)
                pil_img = pil_img.resize((new_w, new_h), Image.LANCZOS)
            
            return pil_img
            
    except Exception as e:
        st.warning(f"Error generating RGB thumbnail for {month_name}: {str(e)}")
        return None


# =============================================================================
# GEE Functions - Simple median composite
# =============================================================================
def create_monthly_composites_list(aoi, start_date, end_date):
    """
    Create a list of month info for downloading.
    Uses simple median composites - matching working app approach.
    """
    start_dt = datetime.datetime.strptime(start_date, '%Y-%m-%d')
    end_dt = datetime.datetime.strptime(end_date, '%Y-%m-%d')
    total_months = (end_dt.year - start_dt.year) * 12 + (end_dt.month - start_dt.month)
    
    composites = []
    
    for month_offset in range(total_months):
        current_date = start_dt + datetime.timedelta(days=month_offset * 30)
        year = current_date.year
        month = current_date.month
        
        month_start = f"{year}-{month:02d}-01"
        if month == 12:
            month_end = f"{year + 1}-01-01"
        else:
            month_end = f"{year}-{month + 1:02d}-01"
        
        month_name = f"{year}-{month:02d}"
        
        composites.append({
            'month_name': month_name,
            'start_date': month_start,
            'end_date': month_end,
            'year': year,
            'month': month
        })
    
    return composites, total_months


# =============================================================================
# Download Functions with .tmp file approach
# =============================================================================
def download_band_with_retry(image, band, aoi, output_path, scale=10):
    """Download a single band with retry mechanism using .tmp file approach."""
    region = aoi.bounds().getInfo()['coordinates']
    
    # If .tmp file exists from previous crash, delete it
    temp_path = output_path + '.tmp'
    if os.path.exists(temp_path):
        os.remove(temp_path)
    
    # If final file exists and is valid, skip
    if os.path.exists(output_path):
        is_valid, msg = validate_band_file(output_path, band)
        if is_valid:
            return True
        else:
            os.remove(output_path)
    
    for attempt in range(MAX_RETRIES):
        try:
            url = image.select(band).getDownloadURL({
                'scale': scale,
                'region': region,
                'format': 'GEO_TIFF',
                'bands': [band]
            })
            
            response = requests.get(url, stream=True, timeout=DOWNLOAD_TIMEOUT)
            
            if response.status_code == 200:
                content_type = response.headers.get('content-type', '')
                if 'text/html' in content_type:
                    raise Exception("Received HTML instead of GeoTIFF - possible rate limit")
                
                # Download to temp file first
                with open(temp_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=CHUNK_SIZE):
                        if chunk:
                            f.write(chunk)
                
                # Validate the downloaded file
                is_valid, msg = validate_band_file(temp_path, band)
                
                if is_valid:
                    # Move temp file to final location (atomic operation)
                    os.replace(temp_path, output_path)
                    return True
                else:
                    if os.path.exists(temp_path):
                        os.remove(temp_path)
                    raise Exception(f"Downloaded file invalid: {msg}")
            else:
                raise Exception(f"HTTP {response.status_code}")
                
        except requests.exceptions.Timeout:
            st.warning(f"‚è±Ô∏è Timeout downloading {band} (attempt {attempt + 1}/{MAX_RETRIES})")
        except requests.exceptions.ConnectionError:
            st.warning(f"üîå Connection error downloading {band} (attempt {attempt + 1}/{MAX_RETRIES})")
        except Exception as e:
            st.warning(f"‚ö†Ô∏è Error downloading {band}: {str(e)} (attempt {attempt + 1}/{MAX_RETRIES})")
        
        # Clean up any partial files
        for f in [output_path, temp_path]:
            if os.path.exists(f):
                try:
                    os.remove(f)
                except:
                    pass
        
        if attempt < MAX_RETRIES - 1:
            wait_time = RETRY_DELAY_BASE ** (attempt + 1)
            st.info(f"‚è≥ Waiting {wait_time}s before retry...")
            time.sleep(wait_time)
    
    return False


def download_monthly_image(aoi, month_info, temp_dir, scale=10, status_placeholder=None):
    """
    Download a single monthly Sentinel-2 composite from GEE.
    Uses simple median composite - matching working app.
    Uses FIXED cloud cover threshold (10%).
    """
    try:
        month_name = month_info['month_name']
        start_date = month_info['start_date']
        end_date = month_info['end_date']
        
        output_file = os.path.join(temp_dir, f"sentinel2_{month_name}.tif")
        
        # Check if already downloaded AND VALID (cache hit)
        if os.path.exists(output_file):
            is_valid, msg = validate_geotiff_file(output_file, expected_bands=len(SPECTRAL_BANDS))
            if is_valid:
                if status_placeholder:
                    status_placeholder.info(f"‚úÖ {month_name} using cached file")
                return output_file
            else:
                if status_placeholder:
                    status_placeholder.warning(f"‚ö†Ô∏è {month_name} cached file corrupted ({msg}), re-downloading...")
                os.remove(output_file)
        
        if status_placeholder:
            status_placeholder.text(f"üì• {month_name}: Searching for images...")
        
        # Simple collection filter with FIXED cloud cover threshold
        collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(aoi)
                     .filterDate(start_date, end_date)
                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', CLOUDY_PIXEL_PERCENTAGE))
                     .select(SPECTRAL_BANDS))
        
        count = collection.size().getInfo()
        
        if count == 0:
            # Try higher cloud cover (up to 30%)
            if status_placeholder:
                status_placeholder.text(f"üì• {month_name}: No images at {CLOUDY_PIXEL_PERCENTAGE}% cloud, trying 30%...")
            
            collection = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                         .filterBounds(aoi)
                         .filterDate(start_date, end_date)
                         .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 30))
                         .select(SPECTRAL_BANDS))
            count = collection.size().getInfo()
            
            if count == 0:
                if status_placeholder:
                    status_placeholder.warning(f"‚ö†Ô∏è {month_name}: No images found")
                return None
        
        if status_placeholder:
            status_placeholder.text(f"üì• {month_name}: Creating median from {count} images...")
        
        # Simple median composite - NO scaling, raw DN values
        median_image = collection.median()
        
        # Download bands
        bands_dir = os.path.join(temp_dir, f"bands_{month_name}")
        os.makedirs(bands_dir, exist_ok=True)
        
        band_files = []
        failed_bands = []
        
        for i, band in enumerate(SPECTRAL_BANDS):
            band_file = os.path.join(bands_dir, f"{band}.tif")
            
            if status_placeholder:
                status_placeholder.text(f"üì• {month_name}: Downloading {band} ({i+1}/{len(SPECTRAL_BANDS)})...")
            
            success = download_band_with_retry(median_image, band, aoi, band_file, scale)
            
            if success:
                band_files.append(band_file)
            else:
                failed_bands.append(band)
        
        if failed_bands:
            st.error(f"‚ùå {month_name}: Failed to download bands: {', '.join(failed_bands)}")
            return None
        
        if len(band_files) == len(SPECTRAL_BANDS):
            if status_placeholder:
                status_placeholder.text(f"üì¶ {month_name}: Creating multiband GeoTIFF...")
            
            with rasterio.open(band_files[0]) as src:
                meta = src.meta.copy()
            
            meta.update(count=len(band_files))
            
            with rasterio.open(output_file, 'w', **meta) as dst:
                for i, band_file in enumerate(band_files):
                    with rasterio.open(band_file) as src:
                        dst.write(src.read(1), i+1)
            
            is_valid, msg = validate_geotiff_file(output_file, expected_bands=len(SPECTRAL_BANDS))
            if not is_valid:
                st.error(f"‚ùå {month_name}: Final multiband file validation failed: {msg}")
                if os.path.exists(output_file):
                    os.remove(output_file)
                return None
            
            return output_file
        else:
            return None
        
    except Exception as e:
        st.error(f"‚ùå Error downloading {month_name}: {str(e)}")
        return None


# =============================================================================
# STRICT NODATA HANDLING - No threshold, any zero/NaN = invalid
# =============================================================================
def check_patch_validity_strict(patch):
    """
    STRICT check if a patch has ANY nodata (NaN or zero values).
    Returns True if patch is COMPLETELY valid (no NaN, no zeros), False otherwise.
    
    This is critical for time series analysis - we need ALL pixels valid in ALL months.
    """
    # Check for any NaN values
    if np.any(np.isnan(patch)):
        return False
    
    # Check for all-zero patch (definite nodata)
    if np.all(patch == 0):
        return False
    
    # Check if ANY band has all zeros (indicates nodata in that band)
    if patch.ndim == 3:
        for band_idx in range(patch.shape[-1]):
            if np.all(patch[:, :, band_idx] == 0):
                return False
    
    # Check for ANY zero values (strict mode - even single zeros indicate potential nodata)
    # This is important because Sentinel-2 valid reflectance values are typically > 0
    if np.any(patch == 0):
        return False
    
    return True


def get_patch_validity_mask_strict(image_path, patch_size=224):
    """
    Create a mask showing which patches are STRICTLY valid (no nodata at all).
    Returns: 2D boolean array where True = completely valid patch
    """
    try:
        with rasterio.open(image_path) as src:
            img_data = src.read()  # (C, H, W)
        
        # Convert to (H, W, C)
        img_for_patching = np.moveaxis(img_data, 0, -1)
        
        h, w, c = img_for_patching.shape
        new_h = int(np.ceil(h / patch_size) * patch_size)
        new_w = int(np.ceil(w / patch_size) * patch_size)
        
        # Pad if needed (padding with zeros will make edge patches invalid, which is correct)
        if h != new_h or w != new_w:
            padded_img = np.zeros((new_h, new_w, c), dtype=img_for_patching.dtype)
            padded_img[:h, :w, :] = img_for_patching
            img_for_patching = padded_img
        
        # Create patches
        patches = patchify(img_for_patching, (patch_size, patch_size, c), step=patch_size)
        
        n_patches_h, n_patches_w = patches.shape[0], patches.shape[1]
        
        # Check each patch with STRICT validation
        validity_mask = np.zeros((n_patches_h, n_patches_w), dtype=bool)
        
        for i in range(n_patches_h):
            for j in range(n_patches_w):
                patch = patches[i, j, 0]
                validity_mask[i, j] = check_patch_validity_strict(patch)
        
        return validity_mask, (h, w), (n_patches_h, n_patches_w)
        
    except Exception as e:
        st.error(f"Error checking validity: {str(e)}")
        return None, None, None


def analyze_months_and_filter(downloaded_images):
    """
    Analyze all downloaded images, count valid patches per month,
    and REMOVE months that have fewer patches than the maximum.
    
    Returns:
        - valid_months: dict of {month_name: image_path} for months that passed
        - deleted_months: list of month names that were removed
        - common_valid_mask: validity mask for patches valid in ALL remaining months
        - original_size: (h, w) of images
        - validity_report: detailed report per month
    """
    st.info("üîç Analyzing patch validity across all months (STRICT mode: any zero/NaN = invalid)...")
    
    month_names = sorted(downloaded_images.keys())
    
    # First pass: count valid patches per month
    validity_data = {}
    original_size = None
    patch_grid_size = None
    
    progress_bar = st.progress(0)
    
    for idx, month_name in enumerate(month_names):
        image_path = downloaded_images[month_name]
        
        validity_mask, orig_size, grid_size = get_patch_validity_mask_strict(image_path, PATCH_SIZE)
        
        if validity_mask is None:
            st.warning(f"Could not check {month_name}")
            continue
        
        if original_size is None:
            original_size = orig_size
            patch_grid_size = grid_size
        
        valid_count = np.sum(validity_mask)
        total_count = validity_mask.size
        
        validity_data[month_name] = {
            'mask': validity_mask,
            'valid_count': valid_count,
            'total_count': total_count,
            'percent': 100 * valid_count / total_count,
            'image_path': image_path
        }
        
        progress_bar.progress((idx + 1) / len(month_names))
    
    progress_bar.empty()
    
    if not validity_data:
        st.error("‚ùå No valid data from any month!")
        return None, [], None, None, []
    
    # Find the MAXIMUM number of valid patches across all months
    max_valid_patches = max(v['valid_count'] for v in validity_data.values())
    
    st.info(f"üìä Maximum valid patches found: {max_valid_patches} out of {patch_grid_size[0] * patch_grid_size[1]} total")
    
    # Separate months into valid (have max patches) and deleted (fewer patches)
    valid_months = {}
    deleted_months = []
    
    for month_name, data in validity_data.items():
        if data['valid_count'] == max_valid_patches:
            valid_months[month_name] = data['image_path']
        else:
            deleted_months.append(month_name)
    
    # Create report
    validity_report = []
    for month_name in month_names:
        if month_name in validity_data:
            data = validity_data[month_name]
            status = "‚úÖ KEPT" if month_name in valid_months else "‚ùå DELETED"
            validity_report.append({
                'month': month_name,
                'valid': data['valid_count'],
                'total': data['total_count'],
                'percent': data['percent'],
                'status': status
            })
    
    # Now find the common valid patches among REMAINING months
    if not valid_months:
        st.error("‚ùå No months passed the validation criteria!")
        return None, deleted_months, None, None, validity_report
    
    # Compute intersection of valid patches across all remaining months
    common_valid_mask = None
    for month_name in valid_months:
        month_mask = validity_data[month_name]['mask']
        if common_valid_mask is None:
            common_valid_mask = month_mask.copy()
        else:
            common_valid_mask = common_valid_mask & month_mask
    
    final_valid_patches = np.sum(common_valid_mask)
    
    # Sanity check: all remaining months should have the same valid patches
    if final_valid_patches != max_valid_patches:
        st.warning(f"‚ö†Ô∏è After intersection, {final_valid_patches} patches are common (expected {max_valid_patches})")
    
    return valid_months, deleted_months, common_valid_mask, original_size, validity_report


def classify_image_with_mask(image_path, model, device, month_name, valid_mask, original_size):
    """
    Classify an image, but only process patches marked as valid.
    Invalid patches are set to 0 (no buildings).
    """
    try:
        with rasterio.open(image_path) as src:
            img_data = src.read()  # (C, H, W)
        
        # Convert to (H, W, C)
        img_for_patching = np.moveaxis(img_data, 0, -1)
        
        h, w, c = img_for_patching.shape
        new_h = int(np.ceil(h / PATCH_SIZE) * PATCH_SIZE)
        new_w = int(np.ceil(w / PATCH_SIZE) * PATCH_SIZE)
        
        # Pad if needed
        if h != new_h or w != new_w:
            padded_img = np.zeros((new_h, new_w, c), dtype=img_for_patching.dtype)
            padded_img[:h, :w, :] = img_for_patching
            img_for_patching = padded_img
        
        # Create patches
        patches = patchify(img_for_patching, (PATCH_SIZE, PATCH_SIZE, c), step=PATCH_SIZE)
        
        n_patches_h, n_patches_w = patches.shape[0], patches.shape[1]
        
        # Initialize output
        classified_patches = np.zeros((n_patches_h, n_patches_w, PATCH_SIZE, PATCH_SIZE), dtype=np.uint8)
        
        valid_count = 0
        skipped_count = 0
        
        for i in range(n_patches_h):
            for j in range(n_patches_w):
                # Check if this patch is valid
                if not valid_mask[i, j]:
                    skipped_count += 1
                    continue  # Leave as zeros
                
                patch = patches[i, j, 0]  # (H, W, C)
                
                # Normalize entire patch (all bands together) - matches working app!
                patch_normalized = normalized(patch)
                
                # Convert to (C, H, W) tensor
                patch_tensor = torch.tensor(np.moveaxis(patch_normalized, -1, 0), dtype=torch.float32)
                patch_tensor = patch_tensor.unsqueeze(0)
                
                # Inference
                with torch.inference_mode():
                    prediction = model(patch_tensor)
                    prediction = torch.sigmoid(prediction).cpu()
                
                pred_np = prediction.squeeze().numpy()
                binary_mask = (pred_np > 0.5).astype(np.uint8) * 255
                
                classified_patches[i, j] = binary_mask
                valid_count += 1
        
        # Reconstruct
        reconstructed = unpatchify(classified_patches, (new_h, new_w))
        reconstructed = reconstructed[:original_size[0], :original_size[1]]
        
        return reconstructed, valid_count, skipped_count
        
    except Exception as e:
        st.error(f"Error classifying {month_name}: {str(e)}")
        return None, 0, 0


# =============================================================================
# Generate Thumbnails
# =============================================================================
def generate_thumbnails(image_path, classification_mask, month_name, max_size=256):
    """Generate both RGB and classification thumbnails."""
    try:
        rgb_thumbnail = generate_rgb_thumbnail(image_path, month_name, max_size)
        
        h, w = classification_mask.shape
        
        if h > max_size or w > max_size:
            scale = max_size / max(h, w)
            new_h, new_w = int(h * scale), int(w * scale)
            pil_class = Image.fromarray(classification_mask.astype(np.uint8))
            pil_class = pil_class.resize((new_w, new_h), Image.NEAREST)
        else:
            pil_class = Image.fromarray(classification_mask.astype(np.uint8))
        
        return {
            'rgb_image': rgb_thumbnail,
            'classification_image': pil_class,
            'month_name': month_name,
            'original_size': (h, w),
            'building_pixels': np.sum(classification_mask > 0),
            'total_pixels': h * w
        }
        
    except Exception as e:
        st.warning(f"Error creating thumbnails for {month_name}: {str(e)}")
        return None


# =============================================================================
# Main Processing Pipeline - THREE PHASES
# =============================================================================
def download_all_images(composites, aoi, temp_dir, scale=10, resume=False):
    """
    PHASE 1: Download all monthly images.
    Supports resuming from previous downloads (cache).
    """
    downloaded_images = {}
    failed_months = []
    
    if resume and st.session_state.downloaded_images:
        downloaded_images = st.session_state.downloaded_images.copy()
        st.info(f"üîÑ Resuming... {len(downloaded_images)} months already downloaded")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, month_info in enumerate(composites):
        month_name = month_info['month_name']
        
        # Skip if already downloaded
        if month_name in downloaded_images:
            # Verify the file still exists and is valid
            cached_path = downloaded_images[month_name]
            if os.path.exists(cached_path):
                is_valid, _ = validate_geotiff_file(cached_path, expected_bands=len(SPECTRAL_BANDS))
                if is_valid:
                    progress_bar.progress((idx + 1) / len(composites))
                    continue
        
        status_text.text(f"üì• Downloading {month_name} ({idx+1}/{len(composites)})...")
        
        image_path = download_monthly_image(
            aoi, month_info, temp_dir,
            scale=scale,
            status_placeholder=status_text
        )
        
        if image_path:
            downloaded_images[month_name] = image_path
        else:
            failed_months.append(month_name)
            st.warning(f"‚ö†Ô∏è Failed to download {month_name}")
        
        progress_bar.progress((idx + 1) / len(composites))
        
        # Save progress to session state
        st.session_state.downloaded_images = downloaded_images
    
    progress_bar.empty()
    status_text.empty()
    
    return downloaded_images, failed_months


def classify_all_images(valid_months, model, device, valid_mask, original_size, resume=False):
    """
    PHASE 3: Classify all validated images using the valid patch mask.
    Supports resuming from previous classifications (cache).
    """
    thumbnails = []
    
    if resume and st.session_state.processed_months:
        already_processed = set(st.session_state.processed_months.keys())
        st.info(f"üîÑ Resuming... {len(already_processed)} months already classified")
    else:
        already_processed = set()
        st.session_state.processed_months = {}
    
    month_names = sorted(valid_months.keys())
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    for idx, month_name in enumerate(month_names):
        # Skip if already processed
        if month_name in already_processed:
            # Add to thumbnails from cache
            if month_name in st.session_state.processed_months:
                thumbnails.append(st.session_state.processed_months[month_name])
            progress_bar.progress((idx + 1) / len(month_names))
            continue
        
        image_path = valid_months[month_name]
        status_text.text(f"üß† Classifying {month_name} ({idx+1}/{len(month_names)})...")
        
        classification_mask, valid_count, skipped_count = classify_image_with_mask(
            image_path, model, device, month_name, valid_mask, original_size
        )
        
        if classification_mask is not None:
            thumbnail_data = generate_thumbnails(image_path, classification_mask, month_name)
            
            if thumbnail_data:
                thumbnail_data['valid_patches'] = valid_count
                thumbnail_data['skipped_patches'] = skipped_count
                thumbnails.append(thumbnail_data)
                st.session_state.processed_months[month_name] = thumbnail_data
        
        progress_bar.progress((idx + 1) / len(month_names))
    
    progress_bar.empty()
    status_text.empty()
    
    return thumbnails


def process_timeseries_strict(aoi, start_date, end_date, model, device, scale=10, resume=False):
    """
    Main processing pipeline with THREE PHASES:
    1. Download all images (with cache)
    2. Analyze patch validity and DELETE months with fewer patches
    3. Classify only valid patches in remaining months (with cache)
    """
    try:
        # Setup temp directory
        if st.session_state.current_temp_dir is None or not os.path.exists(st.session_state.current_temp_dir):
            st.session_state.current_temp_dir = tempfile.mkdtemp()
        temp_dir = st.session_state.current_temp_dir
        
        st.info(f"üìÅ Working directory: {temp_dir}")
        
        # Get month list
        composites, total_months = create_monthly_composites_list(aoi, start_date, end_date)
        st.session_state.total_requested_months = total_months
        st.info(f"üìÖ Requested: {total_months} months")
        
        # =====================================================================
        # PHASE 1: Download all images (with cache support)
        # =====================================================================
        st.header("Phase 1: Downloading Images")
        
        downloaded_images, failed_downloads = download_all_images(
            composites, aoi, temp_dir,
            scale=scale,
            resume=resume
        )
        
        if len(downloaded_images) == 0:
            st.error("‚ùå No images could be downloaded!")
            return []
        
        # Report download results
        st.success(f"‚úÖ Downloaded {len(downloaded_images)}/{total_months} months")
        
        if failed_downloads:
            st.warning(f"‚ö†Ô∏è Failed to download: {', '.join(failed_downloads)}")
            st.session_state.failed_months = failed_downloads
        
        st.session_state.download_phase_complete = True
        
        # =====================================================================
        # PHASE 2: Analyze validity and DELETE months with fewer patches
        # =====================================================================
        st.header("Phase 2: Analyzing Patch Validity & Filtering Months")
        
        valid_months, deleted_months, valid_mask, original_size, validity_report = analyze_months_and_filter(
            downloaded_images
        )
        
        if valid_months is None or len(valid_months) == 0:
            st.error("‚ùå No months passed validation!")
            return []
        
        # Store results
        st.session_state.valid_months = list(valid_months.keys())
        st.session_state.deleted_months = deleted_months
        st.session_state.valid_patches_mask = valid_mask
        st.session_state.original_size = original_size
        st.session_state.validity_analysis_complete = True
        
        # =====================================================================
        # Display Validity Report
        # =====================================================================
        st.subheader("üìä Validity Report by Month")
        
        # Create columns for the report
        report_cols = st.columns(4)
        for idx, report in enumerate(validity_report):
            col_idx = idx % 4
            with report_cols[col_idx]:
                if "KEPT" in report['status']:
                    st.success(f"**{report['month']}**\n{report['valid']}/{report['total']} patches\n{report['percent']:.1f}%")
                else:
                    st.error(f"**{report['month']}**\n{report['valid']}/{report['total']} patches\n{report['percent']:.1f}%")
        
        st.divider()
        
        # Summary statistics
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "Months Requested",
                total_months
            )
        
        with col2:
            st.metric(
                "Months with Valid Data",
                len(valid_months),
                delta=f"-{len(deleted_months)} deleted" if deleted_months else None,
                delta_color="inverse"
            )
        
        with col3:
            valid_patch_count = np.sum(valid_mask) if valid_mask is not None else 0
            total_patches = valid_mask.size if valid_mask is not None else 0
            st.metric(
                "Valid Patches per Month",
                f"{valid_patch_count}/{total_patches}"
            )
        
        # Show deleted months explicitly
        if deleted_months:
            st.warning(f"üóëÔ∏è **Deleted months** (fewer valid patches): {', '.join(deleted_months)}")
        
        # Important summary message
        st.success(f"""
        üìä **Summary**: Out of **{total_months}** months requested, **{len(valid_months)}** months have complete, 
        cloud-free Sentinel-2 data with **{np.sum(valid_mask)}** valid patches each.
        
        These {len(valid_months)} months will be used for time series analysis.
        """)
        
        # Visualize valid patch mask
        if valid_mask is not None:
            fig, ax = plt.subplots(figsize=(8, 6))
            ax.imshow(valid_mask, cmap='RdYlGn', vmin=0, vmax=1)
            ax.set_title(f"Valid Patches Mask\nGreen = Valid in ALL {len(valid_months)} months, Red = Invalid")
            ax.set_xlabel("Patch Column")
            ax.set_ylabel("Patch Row")
            st.pyplot(fig)
        
        # =====================================================================
        # PHASE 3: Classify all validated images (with cache support)
        # =====================================================================
        st.header("Phase 3: Classifying Images")
        
        thumbnails = classify_all_images(
            valid_months, model, device, 
            valid_mask, original_size,
            resume=resume
        )
        
        if thumbnails:
            st.success(f"‚úÖ Successfully classified {len(thumbnails)} months!")
        
        return thumbnails
        
    except Exception as e:
        st.error(f"Error: {str(e)}")
        import traceback
        st.error(traceback.format_exc())
        return []


# =============================================================================
# Display Thumbnails
# =============================================================================
def display_classification_thumbnails(thumbnails):
    """Display RGB and classification thumbnails side by side."""
    
    if not thumbnails:
        st.info("No classifications to display.")
        return
    
    display_mode = st.radio(
        "Display Mode:",
        ["Side by Side (RGB + Classification)", "Classification Only", "RGB Only"],
        horizontal=True
    )
    
    st.divider()
    
    if display_mode == "Side by Side (RGB + Classification)":
        num_cols = 4
        
        for i in range(0, len(thumbnails), 2):
            cols = st.columns(num_cols)
            
            for j in range(2):
                idx = i + j
                if idx < len(thumbnails):
                    thumb = thumbnails[idx]
                    building_pct = (thumb['building_pixels'] / thumb['total_pixels']) * 100
                    
                    with cols[j * 2]:
                        if thumb.get('rgb_image') is not None:
                            st.image(
                                thumb['rgb_image'],
                                caption=f"{thumb['month_name']} (RGB)",
                                use_column_width=True
                            )
                        else:
                            st.warning(f"No RGB for {thumb['month_name']}")
                    
                    with cols[j * 2 + 1]:
                        class_img = thumb.get('classification_image') or thumb.get('image')
                        if class_img is not None:
                            st.image(
                                class_img,
                                caption=f"{thumb['month_name']} ({building_pct:.1f}% buildings)",
                                use_column_width=True
                            )
                        else:
                            st.warning(f"No classification for {thumb['month_name']}")
    
    elif display_mode == "Classification Only":
        num_cols = 4
        num_rows = (len(thumbnails) + num_cols - 1) // num_cols
        
        for row in range(num_rows):
            cols = st.columns(num_cols)
            for col_idx in range(num_cols):
                img_idx = row * num_cols + col_idx
                if img_idx < len(thumbnails):
                    with cols[col_idx]:
                        thumb = thumbnails[img_idx]
                        building_pct = (thumb['building_pixels'] / thumb['total_pixels']) * 100
                        
                        class_img = thumb.get('classification_image') or thumb.get('image')
                        if class_img is not None:
                            st.image(
                                class_img,
                                caption=f"{thumb['month_name']} ({building_pct:.1f}%)",
                                use_column_width=True
                            )
    
    else:  # RGB Only
        num_cols = 4
        num_rows = (len(thumbnails) + num_cols - 1) // num_cols
        
        for row in range(num_rows):
            cols = st.columns(num_cols)
            for col_idx in range(num_cols):
                img_idx = row * num_cols + col_idx
                if img_idx < len(thumbnails):
                    with cols[col_idx]:
                        thumb = thumbnails[img_idx]
                        
                        if thumb.get('rgb_image') is not None:
                            st.image(
                                thumb['rgb_image'],
                                caption=f"{thumb['month_name']} (RGB)",
                                use_column_width=True
                            )


# =============================================================================
# Main Application
# =============================================================================
def main():
    st.title("üèóÔ∏è Building Classification Time Series v06")
    st.markdown("""
    **Features:**
    - ‚úÖ **STRICT nodata handling**: Any zero or NaN pixel = invalid patch
    - ‚úÖ **Month filtering**: Months with fewer valid patches are automatically DELETED
    - ‚úÖ **Consistent time series**: All remaining months have identical valid patch coverage
    - ‚úÖ **Full cache support**: Resume interrupted downloads
    - ‚úÖ **Fixed 10% cloud cover threshold**
    - ‚úÖ **Reports**: Shows how many months have valid cloud-free data
    """)
    
    # Initialize Earth Engine
    ee_initialized, ee_message = initialize_earth_engine()
    
    if not ee_initialized:
        st.error(ee_message)
        st.stop()
    else:
        st.sidebar.success(ee_message)
    
    # Model Loading
    st.sidebar.header("üß† Model Status")
    
    model_path = "best_model_version_Unet++_v02_e7.pt"
    gdrive_model_url = "https://drive.google.com/file/d/1m6EScw-mpBIvWV78h4pyjWq1OLQtn2ov/view?usp=drive_link"
    
    if not os.path.exists(model_path):
        st.sidebar.info("Model not found locally. Downloading...")
        downloaded_path = download_model_from_gdrive(gdrive_model_url, model_path)
        if downloaded_path is None:
            st.sidebar.error("Model download failed. Please upload manually.")
            st.stop()
    
    if not st.session_state.model_loaded:
        with st.spinner("Loading UNet++ model..."):
            model, device = load_model(model_path)
            if model is not None:
                st.session_state.model = model
                st.session_state.device = device
                st.session_state.model_loaded = True
                st.sidebar.success("‚úÖ Model loaded successfully!")
            else:
                st.sidebar.error("‚ùå Failed to load model")
                st.stop()
    else:
        st.sidebar.success("‚úÖ Model already loaded")
    
    # Sidebar - Parameters (no nodata threshold slider - it's always strict)
    st.sidebar.header("‚öôÔ∏è Parameters")
    st.sidebar.info(f"‚òÅÔ∏è Cloud cover threshold: **{CLOUDY_PIXEL_PERCENTAGE}%** (fixed)")
    st.sidebar.info("üîí Nodata handling: **STRICT** (any zero = invalid)")
    
    # Sidebar - Cache Management
    st.sidebar.header("üóÇÔ∏è Cache Management")
    
    cache_info = []
    if st.session_state.downloaded_images:
        cache_info.append(f"üì• {len(st.session_state.downloaded_images)} images downloaded")
    if st.session_state.valid_months:
        cache_info.append(f"‚úÖ {len(st.session_state.valid_months)} months validated")
    if st.session_state.deleted_months:
        cache_info.append(f"üóëÔ∏è {len(st.session_state.deleted_months)} months deleted")
    if st.session_state.processed_months:
        cache_info.append(f"üß† {len(st.session_state.processed_months)} months classified")
    if st.session_state.valid_patches_mask is not None:
        valid_count = np.sum(st.session_state.valid_patches_mask)
        total_count = st.session_state.valid_patches_mask.size
        cache_info.append(f"üìä {valid_count}/{total_count} valid patches")
    
    if cache_info:
        for info in cache_info:
            st.sidebar.success(info)
    else:
        st.sidebar.info("No cached data")
    
    if st.session_state.failed_months:
        st.sidebar.warning(f"‚ùå {len(st.session_state.failed_months)} failed: {', '.join(st.session_state.failed_months)}")
    
    if st.sidebar.button("üóëÔ∏è Clear All Cache"):
        st.session_state.processed_months = {}
        st.session_state.downloaded_images = {}
        st.session_state.failed_months = []
        st.session_state.classification_thumbnails = []
        st.session_state.processing_complete = False
        st.session_state.current_temp_dir = None
        st.session_state.valid_patches_mask = None
        st.session_state.download_phase_complete = False
        st.session_state.validity_analysis_complete = False
        st.session_state.valid_months = []
        st.session_state.deleted_months = []
        st.session_state.total_requested_months = 0
        st.sidebar.success("Cache cleared!")
        st.rerun()
    
    # Region Selection
    st.header("1Ô∏è‚É£ Select Region of Interest")
    
    m = folium.Map(location=[35.6892, 51.3890], zoom_start=8)
    
    draw = plugins.Draw(
        export=True,
        position='topleft',
        draw_options={
            'polyline': False,
            'rectangle': True,
            'polygon': True,
            'circle': False,
            'marker': False,
            'circlemarker': False
        }
    )
    m.add_child(draw)
    
    folium.TileLayer(
        tiles='https://mt1.google.com/vt/lyrs=s&x={x}&y={y}&z={z}',
        attr='Google Satellite',
        name='Google Satellite'
    ).add_to(m)
    
    folium.LayerControl().add_to(m)
    
    map_data = st_folium(m, width=800, height=500)
    
    if map_data is not None and 'last_active_drawing' in map_data and map_data['last_active_drawing'] is not None:
        drawn_shape = map_data['last_active_drawing']
        if 'geometry' in drawn_shape:
            geometry = drawn_shape['geometry']
            if geometry['type'] == 'Polygon':
                coords = geometry['coordinates'][0]
                polygon = Polygon(coords)
                st.session_state.last_drawn_polygon = polygon
                
                centroid = polygon.centroid
                utm_zone = get_utm_zone(centroid.x)
                utm_epsg = get_utm_epsg(centroid.x, centroid.y)
                area_sq_km = polygon.area * 111 * 111
                
                st.success(f"‚úÖ Region captured! UTM Zone {utm_zone} ({utm_epsg}), Area: ~{area_sq_km:.2f} km¬≤")
    
    if st.button("üíæ Save Selected Region"):
        if st.session_state.last_drawn_polygon is not None:
            if not any(p.equals(st.session_state.last_drawn_polygon) for p in st.session_state.drawn_polygons):
                st.session_state.drawn_polygons.append(st.session_state.last_drawn_polygon)
                st.success(f"‚úÖ Region saved!")
    
    # Saved Regions
    if st.session_state.drawn_polygons:
        st.subheader("üìç Saved Regions")
        
        for i, poly in enumerate(st.session_state.drawn_polygons):
            col1, col2, col3, col4 = st.columns([3, 2, 2, 1])
            
            with col1:
                st.write(f"**Region {i+1}**")
            
            with col2:
                centroid = poly.centroid
                st.write(f"UTM: {get_utm_zone(centroid.x)}")
            
            with col3:
                area_sq_km = poly.area * 111 * 111
                st.write(f"Area: ~{area_sq_km:.2f} km¬≤")
            
            with col4:
                if st.button("üóëÔ∏è", key=f"delete_region_{i}"):
                    st.session_state.drawn_polygons.pop(i)
                    st.rerun()
    
    # Date Selection
    st.header("2Ô∏è‚É£ Select Time Period")
    
    col1, col2 = st.columns(2)
    
    with col1:
        start_date = st.date_input(
            "Start Date",
            value=date(2023, 6, 1),
            min_value=date(2017, 1, 1),
            max_value=date.today()
        )
    
    with col2:
        end_date = st.date_input(
            "End Date",
            value=date(2024, 2, 1),
            min_value=date(2017, 1, 1),
            max_value=date.today()
        )
    
    if start_date >= end_date:
        st.error("‚ùå End date must be after start date!")
        st.stop()
    
    num_months = (end_date.year - start_date.year) * 12 + (end_date.month - start_date.month)
    st.info(f"üìÖ Time period: {start_date.strftime('%Y-%m')} to {end_date.strftime('%Y-%m')} ({num_months} months)")
    
    # Process Buttons
    st.header("3Ô∏è‚É£ Generate Building Classifications")
    
    selected_polygon = None
    if len(st.session_state.drawn_polygons) > 0:
        polygon_index = st.selectbox(
            "Select region to process",
            range(len(st.session_state.drawn_polygons)),
            format_func=lambda i: f"Region {i+1} (~{st.session_state.drawn_polygons[i].area * 111 * 111:.2f} km¬≤)",
            key="polygon_selector"
        )
        selected_polygon = st.session_state.drawn_polygons[polygon_index]
    elif st.session_state.last_drawn_polygon is not None:
        selected_polygon = st.session_state.last_drawn_polygon
        st.info("Using the last drawn polygon (not saved)")
    
    # Show current progress
    if st.session_state.downloaded_images:
        st.info(f"üìä Current progress: {len(st.session_state.downloaded_images)} images downloaded, {len(st.session_state.processed_months)} classified")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        start_new = st.button("üöÄ Start New Processing", type="primary")
    
    with col2:
        resume_processing = st.button(
            "üîÑ Resume Processing", 
            disabled=not (st.session_state.downloaded_images or st.session_state.processed_months)
        )
    
    with col3:
        retry_failed = st.button(
            "üîÅ Retry Failed", 
            disabled=not st.session_state.failed_months
        )
    
    should_process = False
    resume_mode = False
    
    if start_new:
        should_process = True
        resume_mode = False
        # Clear cache
        st.session_state.processed_months = {}
        st.session_state.downloaded_images = {}
        st.session_state.failed_months = []
        st.session_state.classification_thumbnails = []
        st.session_state.processing_complete = False
        st.session_state.valid_patches_mask = None
        st.session_state.download_phase_complete = False
        st.session_state.validity_analysis_complete = False
        st.session_state.current_temp_dir = None
        st.session_state.valid_months = []
        st.session_state.deleted_months = []
        st.session_state.total_requested_months = 0
        
    elif resume_processing:
        should_process = True
        resume_mode = True
        
    elif retry_failed:
        should_process = True
        resume_mode = True
        # Only clear failed months, keep successful ones
        st.session_state.failed_months = []
    
    if should_process:
        if selected_polygon is None:
            st.error("‚ùå Please select a region of interest first!")
            st.stop()
        
        if not st.session_state.model_loaded:
            st.error("‚ùå Model not loaded!")
            st.stop()
        
        geojson = {"type": "Polygon", "coordinates": [list(selected_polygon.exterior.coords)]}
        aoi = ee.Geometry.Polygon(geojson['coordinates'])
        
        thumbnails = process_timeseries_strict(
            aoi=aoi,
            start_date=start_date.strftime('%Y-%m-%d'),
            end_date=end_date.strftime('%Y-%m-%d'),
            model=st.session_state.model,
            device=st.session_state.device,
            scale=10,
            resume=resume_mode
        )
        
        if thumbnails:
            st.session_state.classification_thumbnails = thumbnails
            st.session_state.processing_complete = True
    
    # Display Results
    if st.session_state.processing_complete and st.session_state.classification_thumbnails:
        st.divider()
        st.header("üìä Results")
        
        # Show summary
        if st.session_state.valid_months and st.session_state.total_requested_months > 0:
            st.success(f"""
            **Final Summary**: 
            - Requested: {st.session_state.total_requested_months} months
            - Valid (cloud-free): {len(st.session_state.valid_months)} months
            - Deleted: {len(st.session_state.deleted_months)} months
            - Valid patches per month: {np.sum(st.session_state.valid_patches_mask)}/{st.session_state.valid_patches_mask.size}
            """)
        
        display_classification_thumbnails(st.session_state.classification_thumbnails)


if __name__ == "__main__":
    main()
